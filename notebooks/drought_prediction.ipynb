{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒ Somaliland District-Level Drought Prediction Model\n",
    "\n",
    "**Project:** Drought Early Warning System â€” Somaliland  \n",
    "**Author:** Climate Data Science Team  \n",
    "**Version:** 2.0.0  \n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "Build a **district-level drought prediction model** that forecasts drought conditions **3 months ahead** using multi-source climate and socioeconomic data.\n",
    "\n",
    "| Target | Type | Description |\n",
    "|--------|------|-------------|\n",
    "| `target_spi3` | Regression | SPI-3 value at t+3 months |\n",
    "| `target_drought` | Classification | 1 if SPI-3 < -1 at t+3 |\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "| Source | Variables | Resolution |\n",
    "|--------|-----------|------------|\n",
    "| **CHIRPS v2** | Monthly rainfall | 0.05Â° / monthly |\n",
    "| **ERA5-Land** | Temperature, soil moisture, PET | 0.1Â° / monthly |\n",
    "| **FAO SWALIM** | Station rainfall, ASIS index | District / monthly |\n",
    "| **World Bank** | GDP, rural pop, agriculture | National / annual |\n",
    "| **NOAA CPC** | ONI/ENSO index | Global / monthly |\n",
    "\n",
    "## Somaliland Climate Context\n",
    "\n",
    "Somaliland has two rainy seasons:\n",
    "- **Gu** (Aprilâ€“June): Long rains â€” primary agricultural season (~60% of annual rainfall)\n",
    "- **Deyr** (Octoberâ€“November): Short rains â€” secondary season (~25% of annual rainfall)\n",
    "- Dry seasons: **Hagaa** (Julâ€“Sep) and **Jilaal** (Decâ€“Mar)\n",
    "\n",
    "Drought is driven by: ENSO variability, Indian Ocean Dipole, land degradation, and warming trends.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Install required packages (run once)\n",
    "# ============================================================\n",
    "import subprocess, sys\n",
    "\n",
    "packages = [\n",
    "    'pandas', 'numpy', 'matplotlib', 'seaborn', 'scikit-learn',\n",
    "    'xgboost', 'lightgbm', 'shap', 'scipy', 'statsmodels',\n",
    "    'requests', 'geopandas', 'shapely', 'joblib', 'plotly', 'tqdm', 'wbgapi'\n",
    "]\n",
    "for pkg in packages:\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', pkg, '-q'])\n",
    "print('âœ… All packages installed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Imports\n",
    "# ============================================================\n",
    "import sys\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score, mean_absolute_error,\n",
    "    roc_auc_score, classification_report, confusion_matrix,\n",
    "    ConfusionMatrixDisplay, precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "import joblib\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# â”€â”€ Logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# â”€â”€ Reproducibility â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# â”€â”€ Plot style â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 120,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3\n",
    "})\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BASE_DIR    = Path('..').resolve()\n",
    "DATA_DIR    = BASE_DIR / 'data'\n",
    "RAW_DIR     = DATA_DIR / 'raw'\n",
    "PROC_DIR    = DATA_DIR / 'processed'\n",
    "MODELS_DIR  = BASE_DIR / 'models'\n",
    "FIGURES_DIR = BASE_DIR / 'reports' / 'figures'\n",
    "\n",
    "for d in [RAW_DIR, PROC_DIR, MODELS_DIR, FIGURES_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# â”€â”€ Somaliland constants â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "BBOX = dict(min_lon=42.5, max_lon=49.5, min_lat=8.0, max_lat=11.5)\n",
    "\n",
    "DISTRICTS = [\n",
    "    'Hargeisa', 'Berbera', 'Borama', 'Burao', 'Erigavo',\n",
    "    'Las_Anod', 'Gabiley', 'Sheikh', 'Odweyne', 'Zeila'\n",
    "]\n",
    "\n",
    "DISTRICT_COORDS = {\n",
    "    'Hargeisa': (44.065, 9.560), 'Berbera':  (45.014, 10.439),\n",
    "    'Borama':   (43.183, 9.935), 'Burao':    (45.533,  9.517),\n",
    "    'Erigavo':  (47.367, 10.617),'Las_Anod': (47.367,  8.483),\n",
    "    'Gabiley':  (43.467, 9.983), 'Sheikh':   (45.200,  9.933),\n",
    "    'Odweyne':  (45.067, 9.417), 'Zeila':    (43.483, 11.350)\n",
    "}\n",
    "\n",
    "FORECAST_HORIZON  = 3\n",
    "DROUGHT_THRESHOLD = -1.0\n",
    "TRAIN_END   = '2015-12-31'\n",
    "TEST_START  = '2016-01-01'\n",
    "\n",
    "DATE_RANGE = pd.date_range('1981-01', '2023-12', freq='MS')\n",
    "\n",
    "print('âœ… Environment configured.')\n",
    "print(f'   Base dir : {BASE_DIR}')\n",
    "print(f'   Date range: {DATE_RANGE[0].date()} â†’ {DATE_RANGE[-1].date()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Ingestion\n",
    "\n",
    "We pull from five sources. Each has a live API attempt with a graceful fallback to realistic synthetic data (preserving correct climatological statistics) so the notebook always runs end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ src/data_loader.py functions inlined for notebook self-containment â”€â”€\n",
    "\n",
    "def safe_get(url, timeout=20, **kwargs):\n",
    "    \"\"\"HTTP GET with timeout and error handling.\"\"\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=timeout, **kwargs)\n",
    "        r.raise_for_status()\n",
    "        return r\n",
    "    except Exception as e:\n",
    "        logger.warning(f'HTTP failed {url}: {e}')\n",
    "        return None\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2A. CHIRPS â€” Climate Hazards Group InfraRed Precipitation\n",
    "#     URL: https://data.chc.ucsb.edu/products/CHIRPS-2.0/\n",
    "#     Resolution: 0.05Â°, 1981-present\n",
    "#     Production: download monthly GeoTIFFs â†’ zonal stats per district\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def fetch_chirps(districts, date_range, seed=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Fetch CHIRPS monthly rainfall.\n",
    "    Production: download GeoTIFFs and compute zonal statistics.\n",
    "    Fallback: synthetic data with correct bimodal Somaliland climatology.\n",
    "    \"\"\"\n",
    "    # ---- Production download template ----\n",
    "    # for year in range(1981, 2024):\n",
    "    #     for month in range(1, 13):\n",
    "    #         fn = f'chirps-v2.0.{year}.{month:02d}.tif.gz'\n",
    "    #         url = f'https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_monthly/tifs/{fn}'\n",
    "    #         safe_download(url, RAW_DIR / fn)\n",
    "    # then: rasterstats.zonal_stats(district_gdf, tif, stats=['mean'])\n",
    "\n",
    "    logger.info('Generating CHIRPS-like synthetic rainfall ...')\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # Monthly climatology (mm) â€” bimodal Somaliland pattern\n",
    "    #            J    F    M    A    M    J    J    A    S    O    N    D\n",
    "    clim = np.array([5,   8,  18,  55,  90,  60,  12,   8,   5,  45,  60,  10], dtype=float)\n",
    "    mult = dict(Hargeisa=1.0, Berbera=0.4, Borama=1.1, Burao=1.2,\n",
    "                Erigavo=1.5, Las_Anod=1.3, Gabiley=1.1,\n",
    "                Sheikh=1.2, Odweyne=1.1, Zeila=0.6)\n",
    "    rows = []\n",
    "    for d in districts:\n",
    "        m_mult = mult.get(d, 1.0)\n",
    "        for dt in date_range:\n",
    "            mean_r = clim[dt.month - 1] * m_mult\n",
    "            rain = max(0, rng.gamma(1.5, mean_r / 1.5) - 0.04 * (dt.year - 1981))\n",
    "            rows.append({'date': dt, 'district': d, 'rainfall_mm': round(rain, 2)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2B. ERA5-Land â€” ECMWF Reanalysis v5 Land\n",
    "#     Access via CDS API: https://cds.climate.copernicus.eu/\n",
    "#     Requires: ~/.cdsapirc with UID and API key\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def fetch_era5(districts, date_range, seed=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Fetch ERA5-Land monthly climate variables.\n",
    "    Production: use cdsapi.Client() to download NetCDF then xarray for extraction.\n",
    "    Fallback: synthetic ERA5-like data.\n",
    "    \"\"\"\n",
    "    # ---- CDS API template ----\n",
    "    # import cdsapi\n",
    "    # c = cdsapi.Client()\n",
    "    # c.retrieve('reanalysis-era5-land-monthly-means', {\n",
    "    #     'product_type': 'monthly_averaged_reanalysis',\n",
    "    #     'variable': ['2m_temperature', 'volumetric_soil_water_layer_1', 'potential_evaporation'],\n",
    "    #     'year': [str(y) for y in range(1981, 2024)],\n",
    "    #     'month': [f'{m:02d}' for m in range(1, 13)],\n",
    "    #     'time': '00:00',\n",
    "    #     'area': [11.5, 42.5, 8.0, 49.5],  # N,W,S,E\n",
    "    #     'format': 'netcdf'\n",
    "    # }, str(RAW_DIR / 'era5_somaliland.nc'))\n",
    "    # import xarray as xr\n",
    "    # ds = xr.open_dataset(RAW_DIR / 'era5_somaliland.nc')\n",
    "\n",
    "    logger.info('Generating ERA5-like synthetic climate data ...')\n",
    "    rng = np.random.default_rng(seed + 1)\n",
    "    #            J    F    M    A    M    J    J    A    S    O    N    D\n",
    "    t_clim = np.array([23, 24, 26, 27, 28, 28, 25, 25, 26, 26, 25, 23], dtype=float)\n",
    "    sm_clim= np.array([.12,.11,.12,.16,.22,.20,.16,.14,.13,.17,.20,.13], dtype=float)\n",
    "    pt_clim= np.array([110,115,130,135,140,135,120,120,125,120,110,105], dtype=float)\n",
    "    t_off  = dict(Hargeisa=0,Berbera=6,Borama=-1,Burao=1,Erigavo=-2,\n",
    "                  Las_Anod=3,Gabiley=-1,Sheikh=1,Odweyne=1,Zeila=7)\n",
    "    rows = []\n",
    "    for d in districts:\n",
    "        for dt in date_range:\n",
    "            m = dt.month - 1\n",
    "            yr = dt.year - 1981\n",
    "            rows.append({\n",
    "                'date': dt, 'district': d,\n",
    "                'temp_2m_c':  round(t_clim[m] + t_off.get(d,0) + 0.02*yr + rng.normal(0,.8), 2),\n",
    "                'soil_moisture': round(max(0.05, sm_clim[m] + rng.normal(0,.02)), 4),\n",
    "                'pet_mm':     round(max(0, pt_clim[m] + 0.3*yr + rng.normal(0,8)), 2)\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2C. World Bank Open Data via wbgapi\n",
    "#     Country: SO (Somalia â€” Somaliland not separately recognised)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def fetch_world_bank(date_range, seed=RANDOM_SEED):\n",
    "    \"\"\"Fetch World Bank annual socioeconomic indicators.\"\"\"\n",
    "    indicators = {\n",
    "        'AG.AGR.TRVL.ZS': 'agr_value_added_pct',\n",
    "        'SP.RUR.TOTL.ZS':  'rural_pop_pct',\n",
    "        'NY.GDP.PCAP.CD':  'gdp_per_capita',\n",
    "        'AG.LND.ARBL.ZS':  'arable_land_pct',\n",
    "    }\n",
    "    try:\n",
    "        import wbgapi as wb\n",
    "        frames = []\n",
    "        for code, name in indicators.items():\n",
    "            df = wb.data.DataFrame(code, 'SO', time=range(1981, 2024)).T\n",
    "            df.index = pd.to_datetime(df.index.str.replace('YR',''), format='%Y')\n",
    "            df.columns = [name]\n",
    "            frames.append(df)\n",
    "        annual = pd.concat(frames, axis=1)\n",
    "        logger.info('World Bank data fetched via wbgapi.')\n",
    "        return annual\n",
    "    except Exception as e:\n",
    "        logger.warning(f'World Bank API failed: {e}. Using synthetic.')\n",
    "        rng = np.random.default_rng(seed+2)\n",
    "        years = sorted(set(date_range.year))\n",
    "        rows = [{'year': y, 'agr_value_added_pct': 60+rng.normal(0,3)-0.1*i,\n",
    "                 'rural_pop_pct': 72+rng.normal(0,1)-0.15*i,\n",
    "                 'gdp_per_capita': 120+3*i+rng.normal(0,10),\n",
    "                 'arable_land_pct': 1.8+rng.normal(0,.1)}\n",
    "                for i,y in enumerate(years)]\n",
    "        return pd.DataFrame(rows).set_index('year')\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2D. SWALIM / FAO ASIS\n",
    "#     SWALIM: http://www.faoswalim.org/\n",
    "#     ASIS:   https://www.fao.org/giews/earthobservation/\n",
    "#     Both require manual portal download â†’ place CSV in data/raw/\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def fetch_swalim(districts, date_range, seed=RANDOM_SEED):\n",
    "    \"\"\"Load SWALIM station data + FAO ASIS. Falls back to synthetic.\"\"\"\n",
    "    csv_path = RAW_DIR / 'swalim_asis.csv'\n",
    "    if csv_path.exists():\n",
    "        df = pd.read_csv(csv_path, parse_dates=['date'])\n",
    "        logger.info(f'SWALIM data loaded from {csv_path}')\n",
    "        return df\n",
    "    logger.warning('SWALIM CSV not found. Using synthetic ASIS.')\n",
    "    rng = np.random.default_rng(seed+3)\n",
    "    # ASIS 0â€“100; higher = more crop stress; peaks in dry months\n",
    "    asis_clim = np.array([70,65,55,30,20,25,50,55,60,30,25,65], dtype=float)\n",
    "    rows = []\n",
    "    for d in districts:\n",
    "        for dt in date_range:\n",
    "            rows.append({\n",
    "                'date': dt, 'district': d,\n",
    "                'asis_index': round(np.clip(asis_clim[dt.month-1]+rng.normal(0,12),0,100),1)\n",
    "            })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2E. ENSO â€” NOAA ONI (Oceanic NiÃ±o Index)\n",
    "#     ENSO strongly controls Somaliland rainfall:\n",
    "#     La NiÃ±a â†’ drought riskâ†‘; El NiÃ±o â†’ Deyr rainsâ†‘\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def fetch_oni(date_range, seed=RANDOM_SEED):\n",
    "    \"\"\"Download ONI index from NOAA CPC. Fallback: synthetic ENSO.\"\"\"\n",
    "    url = 'https://www.cpc.ncep.noaa.gov/data/indices/oni.ascii.txt'\n",
    "    r = safe_get(url)\n",
    "    if r is not None:\n",
    "        try:\n",
    "            s2m = {'DJF':1,'JFM':2,'FMA':3,'MAM':4,'AMJ':5,'MJJ':6,\n",
    "                   'JJA':7,'JAS':8,'ASO':9,'SON':10,'OND':11,'NDJ':12}\n",
    "            rows = []\n",
    "            for line in r.text.strip().split('\\n')[1:]:\n",
    "                p = line.split()\n",
    "                if len(p) >= 3:\n",
    "                    try: rows.append({'date': pd.Timestamp(year=int(p[1]), month=s2m[p[0]], day=1), 'oni': float(p[2])})\n",
    "                    except: pass\n",
    "            df = pd.DataFrame(rows).drop_duplicates('date').set_index('date')\n",
    "            df = df.reindex(date_range).interpolate().reset_index()\n",
    "            df.columns = ['date','oni']\n",
    "            logger.info('ONI fetched from NOAA.')\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.warning(f'ONI parse failed: {e}')\n",
    "    rng = np.random.default_rng(seed+4)\n",
    "    t = np.arange(len(date_range))\n",
    "    oni = 0.8*np.sin(2*np.pi*t/48) + 0.4*np.sin(2*np.pi*t/84) + rng.normal(0,.3,len(t))\n",
    "    return pd.DataFrame({'date': date_range, 'oni': np.round(oni,3)})\n",
    "\n",
    "\n",
    "# â”€â”€ Load all sources â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "chirps_df  = fetch_chirps(DISTRICTS, DATE_RANGE)\n",
    "era5_df    = fetch_era5(DISTRICTS, DATE_RANGE)\n",
    "swalim_df  = fetch_swalim(DISTRICTS, DATE_RANGE)\n",
    "oni_df     = fetch_oni(DATE_RANGE)\n",
    "wb_df      = fetch_world_bank(DATE_RANGE)\n",
    "\n",
    "print(f'CHIRPS  : {chirps_df.shape}')\n",
    "print(f'ERA5    : {era5_df.shape}')\n",
    "print(f'SWALIM  : {swalim_df.shape}')\n",
    "print(f'ONI     : {oni_df.shape}')\n",
    "print(f'WorldBnk: {wb_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Cleaning & Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all(chirps, era5, swalim, oni, wb):\n",
    "    \"\"\"Merge all data sources into a single master DataFrame.\"\"\"\n",
    "    for df in [chirps, era5, swalim, oni]:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    master = chirps.merge(era5, on=['date','district'], how='left')\n",
    "    master = master.merge(swalim, on=['date','district'], how='left')\n",
    "    master = master.merge(oni, on='date', how='left')\n",
    "\n",
    "    # World Bank: annual â†’ monthly\n",
    "    wb_m = wb.copy()\n",
    "    wb_m.index = pd.to_datetime(wb_m.index, format='%Y')\n",
    "    all_months = pd.date_range(f'{wb_m.index.min().year}-01',\n",
    "                               f'{wb_m.index.max().year}-12', freq='MS')\n",
    "    wb_m = wb_m.reindex(all_months).ffill().bfill()\n",
    "    wb_m.index.name = 'date'\n",
    "    master = master.merge(wb_m.reset_index(), on='date', how='left')\n",
    "\n",
    "    # Calendar features\n",
    "    master['year']      = master['date'].dt.year\n",
    "    master['month']     = master['date'].dt.month\n",
    "    master['month_sin'] = np.sin(2*np.pi*master['month']/12)\n",
    "    master['month_cos'] = np.cos(2*np.pi*master['month']/12)\n",
    "\n",
    "    # Somaliland seasons\n",
    "    def season(m):\n",
    "        return 'Gu' if m in [4,5,6] else 'Deyr' if m in [10,11] \\\n",
    "               else 'Hagaa' if m in [7,8,9] else 'Jilaal'\n",
    "    master['season']   = master['month'].apply(season)\n",
    "    master['is_gu']    = (master['season']=='Gu').astype(int)\n",
    "    master['is_deyr']  = (master['season']=='Deyr').astype(int)\n",
    "\n",
    "    # Spatial features\n",
    "    master['lon'] = master['district'].map(lambda d: DISTRICT_COORDS.get(d,(45,9.5))[0])\n",
    "    master['lat'] = master['district'].map(lambda d: DISTRICT_COORDS.get(d,(45,9.5))[1])\n",
    "\n",
    "    # District code\n",
    "    codes = {d:i for i,d in enumerate(sorted(master['district'].unique()))}\n",
    "    master['district_code'] = master['district'].map(codes)\n",
    "\n",
    "    return master.sort_values(['district','date']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def clean_data(df):\n",
    "    \"\"\"Clean and validate the master dataset.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['rainfall_mm']   = df['rainfall_mm'].clip(lower=0)\n",
    "    df['soil_moisture'] = df['soil_moisture'].clip(0.0, 0.6)\n",
    "    df['asis_index']    = df['asis_index'].clip(0, 100)\n",
    "\n",
    "    # Per-district interpolation (forward-fill up to 2 months, then linear)\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    df[num_cols] = (\n",
    "        df.groupby('district', group_keys=False)[num_cols]\n",
    "        .apply(lambda g: g.ffill(limit=2).bfill(limit=2).interpolate())\n",
    "    )\n",
    "    n0 = len(df)\n",
    "    df = df.dropna(subset=['rainfall_mm'])\n",
    "    if len(df) < n0:\n",
    "        logger.warning(f'Dropped {n0-len(df)} rows with missing rainfall.')\n",
    "    return df\n",
    "\n",
    "\n",
    "master_df = merge_all(chirps_df, era5_df, swalim_df, oni_df, wb_df)\n",
    "master_df = clean_data(master_df)\n",
    "\n",
    "# Quick quality report\n",
    "print(f'Master shape : {master_df.shape}')\n",
    "print(f'Date range   : {master_df[\"date\"].min().date()} â†’ {master_df[\"date\"].max().date()}')\n",
    "print(f'Districts    : {sorted(master_df[\"district\"].unique())}')\n",
    "miss = master_df.isnull().sum()\n",
    "if miss.any(): print('Missing:\\n', miss[miss>0])\n",
    "else: print('âœ… No missing values.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ SPI Computation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def compute_spi(rain_series, scale=3, ref_start=1981, ref_end=2010):\n",
    "    \"\"\"\n",
    "    Standardized Precipitation Index via gamma distribution fitting.\n",
    "\n",
    "    WMO standard method:\n",
    "    1. Accumulate rainfall over `scale` months\n",
    "    2. Per calendar month: fit gamma to 1981-2010 reference period\n",
    "    3. CDF â†’ standard normal quantile = SPI\n",
    "\n",
    "    SPI classes:\n",
    "      â‰¥ +2.0 : Extremely wet\n",
    "      +1.0 to +1.99 : Moderately wet\n",
    "      -0.99 to +0.99: Near normal\n",
    "      -1.0 to -1.49 : Moderate drought\n",
    "      -1.5 to -1.99 : Severe drought\n",
    "      â‰¤ -2.0 : Extreme drought\n",
    "    \"\"\"\n",
    "    rolled = rain_series.rolling(window=scale, min_periods=scale).sum()\n",
    "    spi = pd.Series(np.nan, index=rain_series.index, name=f'spi_{scale}')\n",
    "\n",
    "    if not hasattr(rolled.index, 'month'):\n",
    "        return spi\n",
    "\n",
    "    ref_mask = (rolled.index.year >= ref_start) & (rolled.index.year <= ref_end)\n",
    "\n",
    "    for m in range(1, 13):\n",
    "        m_mask  = rolled.index.month == m\n",
    "        ref_dat = rolled[m_mask & ref_mask].dropna()\n",
    "        all_dat = rolled[m_mask].dropna()\n",
    "        if len(ref_dat) < 8:\n",
    "            continue\n",
    "        # Probability of zero rainfall\n",
    "        q = (ref_dat == 0).mean()\n",
    "        nz = ref_dat[ref_dat > 0]\n",
    "        if len(nz) < 5:\n",
    "            continue\n",
    "        try:\n",
    "            shape, _, sc = stats.gamma.fit(nz, floc=0)\n",
    "            p = (q + (1-q) * stats.gamma.cdf(all_dat, shape, scale=sc)).clip(0.001, 0.999)\n",
    "            spi[all_dat.index] = stats.norm.ppf(p)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return spi\n",
    "\n",
    "\n",
    "def add_spi_all_districts(df, scales=(1, 3, 6)):\n",
    "    \"\"\"Apply SPI computation across all districts and scales.\"\"\"\n",
    "    frames = []\n",
    "    for scale in scales:\n",
    "        col = f'spi_{scale}'\n",
    "        parts = []\n",
    "        for dist, grp in df.groupby('district'):\n",
    "            rain = grp.set_index('date')['rainfall_mm'].sort_index()\n",
    "            s = compute_spi(rain, scale).reset_index()\n",
    "            s.columns = ['date', col]\n",
    "            s['district'] = dist\n",
    "            parts.append(s)\n",
    "        spi_df = pd.concat(parts, ignore_index=True)\n",
    "        frames.append(spi_df)\n",
    "        logger.info(f'SPI-{scale} computed.')\n",
    "    result = df.copy()\n",
    "    for sdf in frames:\n",
    "        col = [c for c in sdf.columns if c.startswith('spi')][0]\n",
    "        result = result.merge(sdf, on=['date','district'], how='left')\n",
    "    return result\n",
    "\n",
    "\n",
    "# â”€â”€ Anomaly Features â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def compute_anomaly(series, month_series, ref_years=(1981,2010)):\n",
    "    \"\"\"Z-score anomaly per calendar month relative to reference period.\"\"\"\n",
    "    anom = pd.Series(np.nan, index=series.index)\n",
    "    ref_mask = ((series.index.year >= ref_years[0]) &\n",
    "                (series.index.year <= ref_years[1]))\n",
    "    for m in range(1, 13):\n",
    "        mm = month_series == m\n",
    "        ref_vals = series[mm & ref_mask]\n",
    "        all_vals = series[mm]\n",
    "        if len(ref_vals) > 2 and ref_vals.std() > 0:\n",
    "            anom[mm] = (all_vals - ref_vals.mean()) / ref_vals.std()\n",
    "    return anom\n",
    "\n",
    "\n",
    "def add_anomaly_features(df):\n",
    "    \"\"\"Add standardized anomaly features per district.\"\"\"\n",
    "    var_map = {\n",
    "        'rainfall_mm':   'rain_anom',\n",
    "        'temp_2m_c':     'temp_anom',\n",
    "        'soil_moisture': 'sm_anom',\n",
    "        'pet_mm':        'pet_anom',\n",
    "    }\n",
    "    df = df.copy()\n",
    "    result_parts = []\n",
    "    for dist, grp in df.groupby('district'):\n",
    "        grp = grp.sort_values('date').copy()\n",
    "        grp = grp.set_index('date')\n",
    "        months = grp['month']\n",
    "        for var, name in var_map.items():\n",
    "            if var in grp.columns:\n",
    "                grp[name] = compute_anomaly(grp[var], months)\n",
    "        result_parts.append(grp.reset_index())\n",
    "    return pd.concat(result_parts, ignore_index=True)\n",
    "\n",
    "\n",
    "# â”€â”€ Lag + Rolling Features â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def add_lag_rolling(df, lag_vars, lags=(1,3,6), windows=(3,6,12)):\n",
    "    \"\"\"\n",
    "    Add lag and rolling mean features per district.\n",
    "    Computed WITHIN each district to prevent spatial leakage.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for dist, grp in df.groupby('district'):\n",
    "        grp = grp.sort_values('date').copy()\n",
    "        for var in lag_vars:\n",
    "            if var not in grp.columns: continue\n",
    "            for lag in lags:\n",
    "                grp[f'{var}_lag{lag}'] = grp[var].shift(lag)\n",
    "            for w in windows:\n",
    "                grp[f'{var}_roll{w}m'] = grp[var].rolling(w, min_periods=max(1,w//2)).mean()\n",
    "        parts.append(grp)\n",
    "    return pd.concat(parts, ignore_index=True)\n",
    "\n",
    "\n",
    "# â”€â”€ Derived / Interaction Features â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def add_derived(df):\n",
    "    \"\"\"Add physically-motivated composite features.\"\"\"\n",
    "    df = df.copy()\n",
    "    # Aridity: P/PET ratio (low = arid)\n",
    "    df['aridity_idx']  = df['rainfall_mm'] / (df['pet_mm'].clip(lower=1))\n",
    "    # Water deficit: PET âˆ’ P (positive = moisture stressed)\n",
    "    df['water_deficit'] = (df['pet_mm'] - df['rainfall_mm']).clip(lower=0)\n",
    "    # Drought stress: high temp Ã— low soil moisture\n",
    "    df['drought_stress'] = df['temp_anom'] * (-df['sm_anom'].clip(upper=0))\n",
    "    # ENSO Ã— season interactions\n",
    "    df['oni_gu']    = df['oni'] * df['is_gu']\n",
    "    df['oni_deyr']  = df['oni'] * df['is_deyr']\n",
    "    # Year-on-year rainfall change\n",
    "    df = df.sort_values(['district','date'])\n",
    "    df['rain_yoy'] = df.groupby('district')['rainfall_mm'].pct_change(12) * 100\n",
    "    return df\n",
    "\n",
    "\n",
    "# â”€â”€ Apply full pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "master_df = add_spi_all_districts(master_df)\n",
    "master_df = add_anomaly_features(master_df)\n",
    "\n",
    "LAG_VARS = [\n",
    "    'rainfall_mm','spi_1','spi_3','temp_2m_c',\n",
    "    'soil_moisture','oni','pet_mm','rain_anom','sm_anom','asis_index'\n",
    "]\n",
    "master_df = add_lag_rolling(master_df, LAG_VARS)\n",
    "master_df = add_derived(master_df)\n",
    "\n",
    "print(f'âœ… Features engineered. Dataset shape: {master_df.shape}')\n",
    "print(f'   Total columns: {len(master_df.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Target Variable Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_targets(df, horizon=FORECAST_HORIZON, threshold=DROUGHT_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Create forecast targets.\n",
    "\n",
    "    CRITICAL: We shift SPI-3 BACKWARDS by `horizon` months so that\n",
    "    features at time t predict conditions at time t+horizon.\n",
    "    Shift is done PER DISTRICT to avoid cross-district contamination.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    for dist, grp in df.groupby('district'):\n",
    "        grp = grp.sort_values('date').copy()\n",
    "        grp['target_spi3']    = grp['spi_3'].shift(-horizon)\n",
    "        grp['target_drought'] = (grp['target_spi3'] < threshold).astype(float)\n",
    "        parts.append(grp)\n",
    "    result = pd.concat(parts, ignore_index=True)\n",
    "    n0 = len(result)\n",
    "    result = result.dropna(subset=['target_spi3'])\n",
    "    result['target_drought'] = result['target_drought'].astype(int)\n",
    "    logger.info(f'Dropped {n0-len(result)} rows (future target NaN).')\n",
    "    logger.info(f'Drought prevalence: {result[\"target_drought\"].mean()*100:.1f}%')\n",
    "    return result\n",
    "\n",
    "\n",
    "master_df = create_targets(master_df)\n",
    "\n",
    "print(f'Target SPI-3 range : [{master_df[\"target_spi3\"].min():.2f}, {master_df[\"target_spi3\"].max():.2f}]')\n",
    "print(f'Drought prevalence : {master_df[\"target_drought\"].mean()*100:.1f}%')\n",
    "print(master_df['target_drought'].value_counts().rename({0:'No Drought',1:'Drought'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Train / Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns: all numeric except identifiers and targets\n",
    "EXCLUDE = {'date','district','season','target_spi3','target_drought','spi_3','spi_6'}\n",
    "FEATURE_COLS = [\n",
    "    c for c in master_df.select_dtypes(include=[np.number]).columns\n",
    "    if c not in EXCLUDE and not c.startswith('target_')\n",
    "]\n",
    "\n",
    "# Remove rows with NaN features (lag/rolling warm-up)\n",
    "model_df = master_df.dropna(subset=FEATURE_COLS + ['target_spi3'])\n",
    "model_df = model_df[model_df['year'] >= 1985]  # 4-year burn-in for lag features\n",
    "\n",
    "train_df = model_df[model_df['date'] <= TRAIN_END].copy()\n",
    "test_df  = model_df[model_df['date'] >= TEST_START].copy()\n",
    "\n",
    "X_train, X_test = train_df[FEATURE_COLS], test_df[FEATURE_COLS]\n",
    "y_train_reg, y_test_reg = train_df['target_spi3'], test_df['target_spi3']\n",
    "y_train_cls, y_test_cls = train_df['target_drought'], test_df['target_drought']\n",
    "\n",
    "print('=' * 55)\n",
    "print('TEMPORAL TRAIN / TEST SPLIT')\n",
    "print('=' * 55)\n",
    "print(f'Train: {train_df[\"date\"].min().date()} â†’ {train_df[\"date\"].max().date()} | {len(train_df):,} samples')\n",
    "print(f'Test : {test_df[\"date\"].min().date()} â†’ {test_df[\"date\"].max().date()} | {len(test_df):,} samples')\n",
    "print(f'Features : {len(FEATURE_COLS)}')\n",
    "print(f'Train drought prevalence: {y_train_cls.mean()*100:.1f}%')\n",
    "print(f'Test  drought prevalence: {y_test_cls.mean()*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Pipeline factory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def make_pipeline(model):\n",
    "    return Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler',  StandardScaler()),\n",
    "        ('model',   model)\n",
    "    ])\n",
    "\n",
    "\n",
    "def eval_reg(name, y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    print(f'  {name:<15s} RMSE={rmse:.4f}  RÂ²={r2:.4f}  MAE={mae:.4f}')\n",
    "    return dict(model=name, rmse=rmse, r2=r2, mae=mae)\n",
    "\n",
    "\n",
    "def eval_cls(name, y_true, y_pred, y_prob):\n",
    "    auc = roc_auc_score(y_true, y_prob) if len(np.unique(y_true))>1 else float('nan')\n",
    "    p,r,f1,_ = precision_recall_fscore_support(y_true,y_pred,average='binary',zero_division=0)\n",
    "    print(f'  {name:<15s} AUC={auc:.4f}  F1={f1:.4f}  P={p:.4f}  R={r:.4f}')\n",
    "    return dict(model=name, auc=auc, f1=f1, precision=p, recall=r)\n",
    "\n",
    "\n",
    "pos_ratio = (y_train_cls==0).sum() / max((y_train_cls==1).sum(), 1)\n",
    "\n",
    "# â”€â”€ Regression â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "REG_MODELS = {\n",
    "    'RandomForest': RandomForestRegressor(\n",
    "        n_estimators=200, max_depth=12, min_samples_leaf=5,\n",
    "        n_jobs=-1, random_state=RANDOM_SEED),\n",
    "    'XGBoost': xgb.XGBRegressor(\n",
    "        n_estimators=300, max_depth=6, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        n_jobs=-1, random_state=RANDOM_SEED, verbosity=0),\n",
    "    'LightGBM': lgb.LGBMRegressor(\n",
    "        n_estimators=300, max_depth=8, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        n_jobs=-1, random_state=RANDOM_SEED, verbose=-1),\n",
    "}\n",
    "\n",
    "# â”€â”€ Classification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "CLS_MODELS = {\n",
    "    'RandomForest': RandomForestClassifier(\n",
    "        n_estimators=200, max_depth=12, min_samples_leaf=5,\n",
    "        class_weight='balanced', n_jobs=-1, random_state=RANDOM_SEED),\n",
    "    'XGBoost': xgb.XGBClassifier(\n",
    "        n_estimators=300, max_depth=6, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        scale_pos_weight=pos_ratio,\n",
    "        n_jobs=-1, random_state=RANDOM_SEED, verbosity=0),\n",
    "    'LightGBM': lgb.LGBMClassifier(\n",
    "        n_estimators=300, max_depth=8, learning_rate=0.05,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1, random_state=RANDOM_SEED, verbose=-1),\n",
    "}\n",
    "\n",
    "# â”€â”€ Train & evaluate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print('REGRESSION â€” SPI-3 (3-month ahead)')\n",
    "print('-' * 55)\n",
    "reg_pipes, reg_res, reg_preds = {}, [], {}\n",
    "for name, mdl in REG_MODELS.items():\n",
    "    pipe = make_pipeline(mdl)\n",
    "    pipe.fit(X_train, y_train_reg)\n",
    "    pred = pipe.predict(X_test)\n",
    "    reg_pipes[name] = pipe\n",
    "    reg_preds[name] = pred\n",
    "    reg_res.append(eval_reg(name, y_test_reg, pred))\n",
    "\n",
    "print()\n",
    "print('CLASSIFICATION â€” Drought binary (3-month ahead)')\n",
    "print('-' * 55)\n",
    "cls_pipes, cls_res, cls_preds, cls_probs = {}, [], {}, {}\n",
    "for name, mdl in CLS_MODELS.items():\n",
    "    pipe = make_pipeline(mdl)\n",
    "    pipe.fit(X_train, y_train_cls)\n",
    "    pred = pipe.predict(X_test)\n",
    "    prob = pipe.predict_proba(X_test)[:,1]\n",
    "    cls_pipes[name] = pipe\n",
    "    cls_preds[name] = pred\n",
    "    cls_probs[name] = prob\n",
    "    cls_res.append(eval_cls(name, y_test_cls, pred, prob))\n",
    "\n",
    "reg_df = pd.DataFrame(reg_res)\n",
    "cls_df = pd.DataFrame(cls_res)\n",
    "best_reg = reg_df.loc[reg_df['rmse'].idxmin(), 'model']\n",
    "best_cls = cls_df.loc[cls_df['f1'].idxmax(), 'model']\n",
    "print(f'\\nğŸ† Best regression model    : {best_reg}')\n",
    "print(f'ğŸ† Best classification model: {best_cls}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Confusion matrix + classification report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "fig.suptitle(f'Classification Evaluation â€” {best_cls}', fontsize=13, fontweight='bold')\n",
    "\n",
    "cm = confusion_matrix(y_test_cls, cls_preds[best_cls])\n",
    "ConfusionMatrixDisplay(cm, display_labels=['No Drought','Drought']).plot(\n",
    "    ax=axes[0], colorbar=False, cmap='Blues')\n",
    "axes[0].set_title('Confusion Matrix')\n",
    "\n",
    "# ROC comparison\n",
    "from sklearn.metrics import roc_curve\n",
    "for name, prob in cls_probs.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test_cls, prob)\n",
    "    auc = roc_auc_score(y_test_cls, prob)\n",
    "    axes[1].plot(fpr, tpr, label=f'{name} (AUC={auc:.3f})')\n",
    "axes[1].plot([0,1],[0,1],'k--',linewidth=0.8,label='Random')\n",
    "axes[1].set_xlabel('False Positive Rate'); axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curves'); axes[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR/'classification_eval.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nClassification Report â€” {best_cls}:')\n",
    "print(classification_report(y_test_cls, cls_preds[best_cls], target_names=['No Drought','Drought']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Feature Importance & SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(pipe, feature_names, title, top_n=25, save_path=None):\n",
    "    model = pipe.named_steps['model']\n",
    "    if not hasattr(model, 'feature_importances_'):\n",
    "        return\n",
    "    fi = pd.DataFrame({'feature': feature_names, 'importance': model.feature_importances_})\n",
    "    fi = fi.sort_values('importance', ascending=False).head(top_n)\n",
    "    colors = plt.cm.RdYlGn_r(np.linspace(0.1, 0.9, len(fi)))\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    ax.barh(fi['feature'][::-1], fi['importance'][::-1], color=colors[::-1])\n",
    "    ax.set_title(title, fontweight='bold', fontsize=13)\n",
    "    ax.set_xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    if save_path: plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    return fi\n",
    "\n",
    "\n",
    "plot_feature_importance(\n",
    "    reg_pipes[best_reg], FEATURE_COLS,\n",
    "    f'Feature Importance â€” {best_reg} (Regression)',\n",
    "    save_path=FIGURES_DIR/'feature_importance_reg.png'\n",
    ")\n",
    "\n",
    "plot_feature_importance(\n",
    "    cls_pipes[best_cls], FEATURE_COLS,\n",
    "    f'Feature Importance â€” {best_cls} (Classification)',\n",
    "    save_path=FIGURES_DIR/'feature_importance_cls.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ SHAP values â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def compute_shap(pipe, X, feature_names, title, max_samples=500, save_path=None):\n",
    "    try:\n",
    "        X_t = pipe[:-1].transform(X.head(max_samples))\n",
    "        explainer = shap.TreeExplainer(pipe.named_steps['model'])\n",
    "        sv = explainer.shap_values(X_t)\n",
    "        if isinstance(sv, list): sv = sv[1]\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.summary_plot(sv, X_t, feature_names=feature_names, show=False, max_display=20)\n",
    "        plt.title(title, fontweight='bold', fontsize=13)\n",
    "        plt.tight_layout()\n",
    "        if save_path: plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        logger.warning(f'SHAP failed: {e}')\n",
    "\n",
    "\n",
    "compute_shap(\n",
    "    reg_pipes[best_reg], X_test, FEATURE_COLS,\n",
    "    f'SHAP Summary â€” {best_reg} (SPI-3 Regression)',\n",
    "    save_path=FIGURES_DIR/'shap_regression.png'\n",
    ")\n",
    "\n",
    "compute_shap(\n",
    "    cls_pipes[best_cls], X_test, FEATURE_COLS,\n",
    "    f'SHAP Summary â€” {best_cls} (Drought Classification)',\n",
    "    save_path=FIGURES_DIR/'shap_classification.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Forecast Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_forecast(test_df, pipe, feature_cols, district, model_name, save_path=None):\n",
    "    \"\"\"Observed vs predicted SPI-3 time series for a single district.\"\"\"\n",
    "    d = test_df[test_df['district']==district].sort_values('date')\n",
    "    y_obs  = d['target_spi3']\n",
    "    y_pred = pipe.predict(d[feature_cols])\n",
    "    dates  = d['date']\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(16, 10), sharex=True)\n",
    "    fig.suptitle(\n",
    "        f'3-Month Ahead Drought Forecast â€” {district} | {model_name}\\n'\n",
    "        f'Test period: {dates.min().date()} â†’ {dates.max().date()}',\n",
    "        fontsize=13, fontweight='bold'\n",
    "    )\n",
    "\n",
    "    ax = axes[0]\n",
    "    ax.plot(dates, y_obs,  color='steelblue', lw=2, label='Observed SPI-3', zorder=3)\n",
    "    ax.plot(dates, y_pred, color='orangered', lw=2, ls='--', label='Predicted SPI-3', zorder=3)\n",
    "    ax.axhline(-1, color='red', ls=':', lw=1.5, label='Drought threshold')\n",
    "    ax.axhline(0,  color='gray', ls='-', lw=0.5)\n",
    "    ax.fill_between(dates, -4, 4, where=(y_obs<-1), alpha=0.15, color='red', label='Drought')\n",
    "    ax.set_ylim(-3.5, 3.5); ax.set_ylabel('SPI-3'); ax.legend(fontsize=9)\n",
    "\n",
    "    resid = y_obs.values - y_pred\n",
    "    ax2 = axes[1]\n",
    "    ax2.bar(dates, resid, color=np.where(resid>0,'steelblue','tomato'), alpha=0.75)\n",
    "    ax2.axhline(0, color='black', lw=0.8)\n",
    "    ax2.set_ylabel('Residual (Obsâˆ’Pred)')\n",
    "    ax2.set_title(f'RMSE = {np.sqrt(mean_squared_error(y_obs, y_pred)):.3f}', fontsize=11)\n",
    "    ax2.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path: plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for dist in ['Hargeisa', 'Erigavo', 'Berbera']:\n",
    "    plot_forecast(\n",
    "        test_df, reg_pipes[best_reg], FEATURE_COLS,\n",
    "        dist, best_reg,\n",
    "        save_path=FIGURES_DIR/f'forecast_{dist.lower()}.png'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Correlation heatmap â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "key_cols = ['rainfall_mm','spi_1','spi_3','temp_2m_c','soil_moisture',\n",
    "            'pet_mm','asis_index','oni','rain_anom','sm_anom',\n",
    "            'water_deficit','aridity_idx','target_spi3']\n",
    "key_cols = [c for c in key_cols if c in master_df.columns]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 11))\n",
    "corr = master_df[key_cols].corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='RdBu_r',\n",
    "            center=0, vmin=-1, vmax=1, square=True, linewidths=.5,\n",
    "            ax=ax, annot_kws={'size': 8})\n",
    "ax.set_title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR/'correlation_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Seasonal analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Somaliland Seasonal Climate Analysis', fontsize=14, fontweight='bold')\n",
    "s_colors = {'Gu':'#27AE60','Deyr':'#2980B9','Hagaa':'#E67E22','Jilaal':'#E74C3C'}\n",
    "\n",
    "# Seasonal rainfall boxplot\n",
    "s_order = ['Gu','Deyr','Hagaa','Jilaal']\n",
    "data_s  = [master_df[master_df['season']==s]['rainfall_mm'].dropna() for s in s_order]\n",
    "bp = axes[0,0].boxplot(data_s, labels=s_order, patch_artist=True)\n",
    "for patch, s in zip(bp['boxes'], s_order):\n",
    "    patch.set_facecolor(s_colors[s]); patch.set_alpha(0.75)\n",
    "axes[0,0].set_title('Rainfall by Season'); axes[0,0].set_ylabel('mm/month')\n",
    "\n",
    "# Monthly climatology\n",
    "mc = master_df.groupby('month')['rainfall_mm'].mean()\n",
    "m_colors = ['#27AE60' if m in [4,5,6] else '#2980B9' if m in [10,11] else '#E74C3C' for m in range(1,13)]\n",
    "axes[0,1].bar(range(1,13), mc.values, color=m_colors, edgecolor='white')\n",
    "axes[0,1].set_xticks(range(1,13))\n",
    "axes[0,1].set_xticklabels(['J','F','M','A','M','J','J','A','S','O','N','D'])\n",
    "axes[0,1].set_title('Monthly Climatology'); axes[0,1].set_ylabel('Mean Rainfall (mm)')\n",
    "\n",
    "# Drought frequency by district\n",
    "df_by_d = (master_df.groupby('district').apply(lambda g: (g['spi_3']<-1).mean()*100)\n",
    "           .sort_values())\n",
    "c_d = plt.cm.RdYlGn_r(np.linspace(.1,.9,len(df_by_d)))\n",
    "axes[1,0].barh(df_by_d.index, df_by_d.values, color=c_d)\n",
    "axes[1,0].set_title('Drought Frequency by District'); axes[1,0].set_xlabel('%')\n",
    "\n",
    "# Annual trend\n",
    "ann = master_df.groupby('year').apply(lambda g: (g['spi_3']<-1).mean()*100)\n",
    "axes[1,1].plot(ann.index, ann.values, color='tomato', lw=2, marker='o', ms=3)\n",
    "axes[1,1].fill_between(ann.index, 0, ann.values, color='tomato', alpha=0.2)\n",
    "z = np.polyfit(ann.index, ann.values, 1)\n",
    "axes[1,1].plot(ann.index, np.poly1d(z)(ann.index), 'k--', lw=1.5, label=f'{z[0]:+.2f}%/yr')\n",
    "axes[1,1].set_title('Annual Drought Trend'); axes[1,1].set_ylabel('Districts in Drought (%)'); axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR/'seasonal_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Interactive drought map â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "drought_freq = (\n",
    "    master_df.groupby('district')\n",
    "    .apply(lambda g: (g['spi_3']<-1).mean()*100)\n",
    "    .reset_index(name='drought_pct')\n",
    ")\n",
    "drought_freq['lon'] = drought_freq['district'].map(lambda d: DISTRICT_COORDS.get(d,(45,9.5))[0])\n",
    "drought_freq['lat'] = drought_freq['district'].map(lambda d: DISTRICT_COORDS.get(d,(45,9.5))[1])\n",
    "\n",
    "fig = px.scatter_mapbox(\n",
    "    drought_freq, lat='lat', lon='lon',\n",
    "    color='drought_pct', size='drought_pct',\n",
    "    hover_name='district', color_continuous_scale='RdYlGn_r',\n",
    "    size_max=45, zoom=6, center=dict(lat=9.8, lon=45.5),\n",
    "    mapbox_style='open-street-map',\n",
    "    title='Drought Frequency â€” Somaliland Districts (1981â€“2023)'\n",
    ")\n",
    "fig.update_layout(height=520)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Save Models & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all pipelines\n",
    "for name, pipe in reg_pipes.items():\n",
    "    p = MODELS_DIR / f'reg_{name.lower()}.pkl'\n",
    "    joblib.dump(pipe, p, compress=3)\n",
    "    print(f'Saved: {p.name} ({p.stat().st_size/1024:.0f} KB)')\n",
    "\n",
    "for name, pipe in cls_pipes.items():\n",
    "    p = MODELS_DIR / f'cls_{name.lower()}.pkl'\n",
    "    joblib.dump(pipe, p, compress=3)\n",
    "    print(f'Saved: {p.name} ({p.stat().st_size/1024:.0f} KB)')\n",
    "\n",
    "# Save feature list\n",
    "joblib.dump(FEATURE_COLS, MODELS_DIR / 'feature_columns.pkl')\n",
    "\n",
    "# Save processed data\n",
    "master_df.to_csv(PROC_DIR / 'features_full.csv', index=False)\n",
    "train_df.to_csv(PROC_DIR / 'train.csv', index=False)\n",
    "test_df.to_csv(PROC_DIR / 'test.csv', index=False)\n",
    "reg_df.to_csv(PROC_DIR / 'regression_results.csv', index=False)\n",
    "cls_df.to_csv(PROC_DIR / 'classification_results.csv', index=False)\n",
    "\n",
    "print('\\nâœ… All models and data saved.')\n",
    "print(f'   Models   â†’ {MODELS_DIR}')\n",
    "print(f'   Data     â†’ {PROC_DIR}')\n",
    "print(f'   Figures  â†’ {FIGURES_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Results\n",
    "\n",
    "| Task | Best Model | Key Metric |\n",
    "|------|-----------|------------|\n",
    "| SPI-3 Regression | â€” | RMSE / RÂ² |\n",
    "| Drought Classification | â€” | AUC / F1 |\n",
    "\n",
    "### Next Steps\n",
    "- Replace synthetic data with real CHIRPS rasters + CDS API ERA5 download\n",
    "- Add cross-validation using blocked time-series CV (`TimeSeriesSplit`)\n",
    "- Tune hyperparameters with Optuna\n",
    "- Deploy as a Streamlit dashboard (see `streamlit_app/`)\n",
    "- Set up automated monthly retraining pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
